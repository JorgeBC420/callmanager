# ü§ñ Gu√≠a de Configuraci√≥n: Continue + Ollama (Offline)

## Estado Actual

‚úÖ **Configuraci√≥n completada:**
- Archivo `config.yaml` creado en `C:\Users\bjorg\.continue\`
- 4 modelos Ollama configurados
- CallManager v2.0 listo

## Modelos Disponibles (Offline)

```yaml
1. DeepSeek V3     ‚Üí deepseek-v3      (Muy r√°pido, excelente c√≥digo)
2. DeepSeek R1     ‚Üí deepseek-r1      (Razonamiento, buenas explicaciones)
3. Llama 3.2 11B   ‚Üí llama3.2-11b     (Ligero, r√°pido)
4. GPT OSS 120B    ‚Üí gpt-oss-120b     (M√°s pesado pero poderoso)
```

## ‚öôÔ∏è C√≥mo Activar Continue Offline

### Paso 1: Iniciar Ollama
```powershell
ollama serve
```
Debe mostrar:
```
Listening on 127.0.0.1:11434
```

### Paso 2: Abrir VS Code
- Presiona `Ctrl + K` (Abre Quick Commands)
- Escribe: `Continue`
- Selecciona: `Continue: Open Chat`

### Paso 3: Seleccionar Modelo
- En el chat de Continue, busca el dropdown de modelos
- Selecciona uno de los 4 modelos Ollama disponibles
- Ej: "DeepSeek V3 (Offline)"

### Paso 4: ¬°A Usar!
```
[Chat Input] ¬øC√≥mo optimizo esta funci√≥n?
[Continue]   Usa el modelo offline seleccionado
```

## üîß Troubleshooting

### ‚ùå "Cannot connect to Ollama"
```powershell
# Verifica que Ollama est√© ejecut√°ndose
ollama list

# Si no ve modelos, desc√°rgate uno:
ollama pull deepseek-v3
```

### ‚ùå "No models available"
```powershell
# Aseg√∫rate de que est√° en C:\Users\bjorg\.continue\config.yaml
Get-Content "$env:USERPROFILE\.continue\config.yaml"
```

### ‚ùå Continue no aparece en VS Code
```
1. Instala extensi√≥n: "Continue - Coding with AI"
2. Recarga VS Code: Ctrl+Shift+P ‚Üí Developer: Reload Window
3. Abre Continue: Ctrl+Shift+C
```

## üìã Configuraci√≥n Actual

**Archivo:** `C:\Users\bjorg\.continue\config.yaml`

```yaml
models:
  - provider: "ollama"
    apiBase: "http://localhost:11434"
    models: 
      - deepseek-v3
      - deepseek-r1
      - llama3.2-11b
      - gpt-oss-120b
```

## üöÄ Ventajas de Usar Offline

‚úÖ **Sin l√≠mites de uso**
- Chats ilimitados
- Sin cuota de API
- Sin throttling

‚úÖ **Privacidad**
- Tu c√≥digo nunca sale de tu PC
- Informaci√≥n sensible protegida
- Funciona sin conexi√≥n a internet

‚úÖ **Velocidad**
- M√°s r√°pido que esperar por API remota
- CPU/GPU local optimizadas
- Ideal para desarrollo local

## üí° Tips de Uso

**Para Coding:**
```
"DeepSeek V3" - Mejor para c√≥digo Python
"DeepSeek R1" - Mejor para debugging
```

**Para Documentaci√≥n:**
```
"Llama 3.2 11B" - M√°s r√°pido para docs
```

**Para An√°lisis Profundo:**
```
"GPT OSS 120B" - M√°s potente pero lento
```

## ‚úÖ Checklist

- [x] Configuraci√≥n de Continue creada
- [x] Modelos Ollama agregados a config.yaml
- [x] Archivo copiado a ~/.continue/
- [ ] Ollama instalado y ejecut√°ndose (`ollama serve`)
- [ ] Extensi√≥n Continue instalada en VS Code
- [ ] Probado con al menos 1 modelo

## üìù Para la Pr√≥xima Sesi√≥n

1. **Inicia Ollama:**
   ```powershell
   ollama serve
   ```

2. **Abre VS Code y prueba Continue:**
   - Ctrl+Shift+C
   - Selecciona un modelo
   - ¬°Disfruta coding con IA offline!

---

**√öltima actualizaci√≥n:** 21 Noviembre 2025
**Estado:** ‚úÖ Lista para usar
