#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Setup script para configurar Continue con Ollama
"""

import os
import shutil
import sys
from pathlib import Path

def setup_continue_config():
    """Copiar configuraci√≥n de Continue a la carpeta correcta"""
    
    # Directorio de Continue
    continue_dir = Path.home() / ".continue"
    config_file = continue_dir / "config.yaml"
    
    # Archivo local de configuraci√≥n
    local_config = Path(__file__).parent / ".continue_config.yaml"
    
    print(f"üìÅ Directorio Continue: {continue_dir}")
    print(f"üìÑ Configuraci√≥n local: {local_config}")
    
    # Crear directorio si no existe
    continue_dir.mkdir(exist_ok=True)
    print(f"‚úì Directorio creado/verificado")
    
    # Copiar archivo
    try:
        if local_config.exists():
            shutil.copy2(local_config, config_file)
            print(f"‚úÖ Configuraci√≥n copiada a: {config_file}")
        else:
            print(f"‚ùå Archivo local no encontrado: {local_config}")
            return False
            
    except Exception as e:
        print(f"‚ùå Error al copiar: {e}")
        return False
    
    # Verificar que se copi√≥ correctamente
    if config_file.exists():
        print(f"‚úÖ Verificaci√≥n OK - Archivo existe")
        with open(config_file, 'r', encoding='utf-8') as f:
            print(f"\nüìã Contenido de configuraci√≥n:")
            print(f.read())
        return True
    else:
        print(f"‚ùå Verificaci√≥n FALLIDA - Archivo no existe")
        return False

def check_ollama():
    """Verificar si Ollama est√° disponible"""
    print("\n" + "="*60)
    print("VERIFICAR OLLAMA")
    print("="*60)
    
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        if response.status_code == 200:
            print("‚úÖ Ollama est√° ejecut√°ndose en localhost:11434")
            models = response.json().get("models", [])
            print(f"\nüì¶ Modelos disponibles ({len(models)}):")
            for model in models:
                print(f"   ‚Ä¢ {model.get('name')}")
            return True
        else:
            print(f"‚ùå Error HTTP {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("‚ùå Ollama no est√° ejecut√°ndose en localhost:11434")
        print("\nüí° Para iniciar Ollama, ejecuta:")
        print("   ollama serve")
        return False
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return False

if __name__ == "__main__":
    print("="*60)
    print("SETUP CONTINUE + OLLAMA")
    print("="*60)
    
    # Setup de configuraci√≥n
    if setup_continue_config():
        print("\n‚úÖ Configuraci√≥n de Continue lista")
    else:
        print("\n‚ùå Error en configuraci√≥n de Continue")
        sys.exit(1)
    
    # Verificar Ollama
    if not check_ollama():
        print("\n‚ö†Ô∏è  IMPORTANTE: Ollama no est√° ejecut√°ndose")
        print("   Inicia Ollama con: ollama serve")
    
    print("\n" + "="*60)
    print("‚úÖ SETUP COMPLETADO")
    print("="*60)
    print("\nüìù Pr√≥ximos pasos:")
    print("   1. Aseg√∫rate de que Ollama est√© ejecut√°ndose: ollama serve")
    print("   2. Abre VS Code")
    print("   3. Presiona Ctrl+Shift+C para abrir Continue")
    print("   4. Selecciona uno de los modelos Ollama disponibles")
